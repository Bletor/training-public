{
 "metadata": {
  "name": "",
  "signature": "sha256:9b700cd83fb5b52591fcbfe03ec1d1fd04a7570a58372aa20c321d4ececf10d0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Stochastic Differential Equations - Lecture 2"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Kostas Zygalakis"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Reminders"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A stochastic integral is\n",
      "\n",
      "$$\n",
      "\\begin{equation}\n",
      "  \\int_{t_0}^t G(t') \\, dW(t') = \\text{mean-square-}\\lim_{n\\to\\infty} \\left\\{ \\sum_{i=1}^n G(t_{i-1}) \\left( W_{t_i} - W_{t_{i-1}} \\right) \\right\\}\n",
      "\\end{equation}\n",
      "$$\n",
      "\n",
      "where $W$ (sometimes denoted $B$) is a *Wiener process* or *Brownian motion* or *Brownian path*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The numerical solution of SDEs is given by\n",
      "\n",
      "$$\n",
      "\\begin{equation}\n",
      "  dX_t = f(X_t) \\, dt + g(X_t) \\, dW_t.\n",
      "\\end{equation}\n",
      "$$\n",
      "\n",
      "This is *not* the same as\n",
      "\n",
      "$$\n",
      "\\begin{equation}\n",
      "  \\frac{d X_t}{dt} = f(X_t) + g(X_t) \\frac{d W_t}{dt}\n",
      "\\end{equation}\n",
      "$$\n",
      "\n",
      "as the path itself is not differentiable. Instead it should be interpreted as the integral equation\n",
      "\n",
      "$$\n",
      "\\begin{equation}\n",
      "  X_t = X_0 + \\int_0^t f(X(s)) \\, ds + \\int_0^t g(X(s)) \\, dW_s.\n",
      "\\end{equation}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Doing this discretely we have\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "  X((t+1)h) - X(th) & = X_0 + \\int_0^{(t+1)h)} f(X(s)) \\, ds + \\int_0^{(t+1)h} g(X(s)) \\, dW_s - X_0 - \\int_0^{th} f(X(s)) \\, ds - \\int_0^{th} g(X(s)) \\, dW_s \\\\\n",
      "  & = \\int_{th}^{(t+1)h)} f(X(s)) \\, ds + \\int_{th}^{(t+1)h} g(X(s)) \\, dW_s \\\\\n",
      "  & \\simeq f(X(th)) \\left[ (t+1)h - th \\right] + g(X(th)) \\left[ W \\left( (t+1) h \\right) - W(th) \\right] \\\\\n",
      "  & \\simeq f(X(th)) h + g(X(th)) \\xi_n.\n",
      "\\end{align}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So the algorithm becomes\n",
      "\n",
      "$$\n",
      "\\begin{equation}\n",
      "  X_{n+1} - X_n = f(X_n) h + g(X_n) \\xi_n\n",
      "\\end{equation}\n",
      "$$\n",
      "\n",
      "where $\\xi_n$ is a random variable with\n",
      "\n",
      "1. zero mean,\n",
      "2. normally distributed,\n",
      "3. varaiance $(t+1)h - th = h$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is often rewritten as (the **Euler-Maruyama** method)\n",
      "\n",
      "$$\n",
      "\\begin{equation}\n",
      "  X_{n+1} - X_n = f(X_n) h + \\sqrt{h} g(X_n) \\hat{\\xi}_n, \\qquad \\hat{\\xi}_n \\sim N(0, 1).\n",
      "\\end{equation}\n",
      "$$\n",
      "\n",
      "The random variable $\\hat{\\xi}_n$ is therefore the standard normally distributed variable with mean zero and variance (or standard deviation) one."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Accuracy and convergence"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider the simple ODE\n",
      "\n",
      "$$\n",
      "\\begin{equation}\n",
      "  \\frac{dX}{dt} = f(X(t)).\n",
      "\\end{equation}\n",
      "$$\n",
      "\n",
      "Given the simple Euler method\n",
      "\n",
      "$$\n",
      "\\begin{equation}\n",
      "  X_{n+1} = X_n + h f(X_n)\n",
      "\\end{equation}\n",
      "$$\n",
      "\n",
      "we have that the error (given the true solution $X^{(e)}(T)$ and the numerical solution $X^{(h)}(T)$ with step size $h$ at identical times $T$) is\n",
      "\n",
      "$$\n",
      "\\begin{equation}\n",
      "  | X^{(e)}(T) - X^{(h)}(T) | \\le C(T) h^{\\alpha} \n",
      "\\end{equation}\n",
      "$$\n",
      "\n",
      "where $\\alpha = 1$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**NOTE**: picture here looking at the difference between the exact integral and the \"Euler\" integral over the range of $h$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Argument for the convergence result:\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "  && \\frac{d f(X(t))}{dt} & = \\frac{df}{dX} \\frac{dX}{dt} \\\\\n",
      "  \\implies && \\frac{df}{dt} & = f'(X(t))f(X(t)) \\\\\n",
      "  \\implies && f(X(t)) = f(X(0)) + \\int_0^t f'(X(s)) f(X(s)) \\, ds.\n",
      "\\end{align}\n",
      "$$\n",
      "\n",
      "Hence we have\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "  \\int_0^h f(X(s)) \\, ds & = \\int_0^h \\left[ f(X(0)) + \\left( \\int_0^s f'(X(l))f(X(l)) \\right) \\, dl \\right] \\, ds \\\\\n",
      "  & = h f(X(0)) + \\int_0^h \\left( \\int_0^s f'(X(l)) f(X(l)) \\, dl \\right) \\, ds\n",
      "\\end{align}\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Extending the analysis to SDEs"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Version 1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\n",
      "\\begin{equation}\n",
      "  X_{n+1} = X_n + h f(X_n) + g(X_n) \\Delta W_n\n",
      "\\end{equation}\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Version 2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\n",
      "\\begin{equation}\n",
      "  X_{n+1} = X_n + h f(X_n) + \\sqrt{h} g(X_n) \\xi_n, \\qquad \\xi_n \\sim N(0, 1).\n",
      "\\end{equation}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Measuring the errors is more difficult as we have a random variable, so each run will give different results. Instead we must *average*."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Strong convergence"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this case we measure\n",
      "\n",
      "$$\n",
      "\\begin{equation}\n",
      "  \\mathbb{E} | X^{(e)}(T) - X^{(h)}(T) | \\le C h^{\\gamma}.\n",
      "\\end{equation}\n",
      "$$\n",
      "\n",
      "This is the **mean error**. This *means* that we are assuming an *underlying* random process, or a *fixed* Brownian process, which is *approximated* by $\\xi_n$ or by $\\Delta W_n$. As we vary the step size $h$ we keep the same underlying Brownian path, just evaluate it differently (effectively integrating over segments of different lengths)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The expectation here is that the convergence rate for Euler-Maruyama here is $\\gamma = 1/2$, thanks to the $\\sqrt{h}$ factor in the random process. This \"intuitively\" follows from thinking about generalizing the above analysis to the stochastic case, looking at the extra terms in the process, and can easily be checked numerically."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Weak convergence"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this case we measure\n",
      "\n",
      "$$\n",
      "\\begin{equation}\n",
      "  \\left| \\mathbb{E} (p(X^{(e)}(T)) - \\mathbb{E} (p(X^{(h)}(T)) \\right| \\le C h^{\\gamma}.\n",
      "\\end{equation}\n",
      "$$\n",
      "\n",
      "Here $p$ is some function of the solution. This is the **error of the means**. For Euler-Maruyama the result is that $\\gamma = 1$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you consider $p(X) = X$ we can compare the weak convergence test\n",
      "\n",
      "$$\n",
      "\\begin{equation}\n",
      "  \\left| \\mathbb{E} (X^{(e)}(T)) - \\mathbb{E} (X^{(h)}(T)) \\right| \n",
      "\\end{equation}\n",
      "$$\n",
      "\n",
      "with the strong\n",
      "\n",
      "$$\n",
      "\\begin{equation}\n",
      "  \\mathbb{E} \\left| X^{(e)}(T) - X^{(h)}(T) \\right| .\n",
      "\\end{equation}\n",
      "$$\n",
      "\n",
      "By considering two Gaussian random processes we can see that strong convergence implies weak convergence, but not the other way around (two processes with identical mean need not have the same realization). "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Stochastic chain rule"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Without proof:\n",
      "\n",
      "$$\n",
      "\\begin{equation}\n",
      "  dW^2 = dt, \\qquad dW^{2+N} = 0, \\quad \\text{for } N > 0.\n",
      "\\end{equation}\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Deterministic chain rule"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\n",
      "\\begin{equation}\n",
      "  \\frac{dX}{dt} = f(X).\n",
      "\\end{equation}\n",
      "$$\n",
      "\n",
      "Hence\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "  \\frac{d}{dt} V(X_t) & \\simeq V(X(t) + dX(t)) - V(X(t)) \\\\\n",
      "  & \\simeq V(X(t)) + V'(X(t)) \\, dX(t) + \\frac{1}{2} V''(X(t)) \\, (dX(t))^2 - V(x(t)) \\\\\n",
      "  & \\simeq V'(X(t)) \\, dX(t) + \\frac{1}{2} V''(X(t)) \\, (dX(t))^2 \\\\\n",
      "  & \\simeq V'(X) f(X)\\, dt + \\underbrace{\\frac{1}{2} V''(X) f^2(X) \\, dt^2}_{\\text{small}} \\\\\n",
      "  \\frac{dV}{dt} & = V'(X) f(X).\n",
      "\\end{align}\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Stochastic case"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\n",
      "\\begin{align}\n",
      "  dV(x) & = V(X(t) + dX(t)) - V(X(t)) \\\\\n",
      "  & \\simeq V'(X) \\, dX + \\frac{1}{2} V''(X) \\, (dX)^2 \\\\\n",
      "  & \\simeq V'(X) \\left[ f(X)\\,dt + g(X)\\,dW_t \\right] + \\frac{1}{2} V''(X) \\left[ f(X)\\,dt + g(X)\\,dW_t \\right]^2 \\\\\n",
      "  & \\simeq V'(X) \\left[ f(X)\\,dt + g(X)\\,dW_t \\right] + \\frac{1}{2} V''(X) \\left[ f(X)^2\\,dt^2 + f(X) g(X)\\,dt dW_t + g^2\\,dW^2_t \\right] \\\\\n",
      "  & \\simeq \\left[ V' f + \\frac{1}{2} V'' g^2 \\right] \\, dt + V' g \\, dW_t + \\text{higher order terms}.\n",
      "\\end{align}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that we expect additional terms resulting from the Brownian path."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Application"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remember the stochastic integral\n",
      "\n",
      "$$\n",
      "\\begin{equation}\n",
      "  \\int_{t_0}^t W_s \\, dW_s = \\frac{1}{2} W_t^2 - \\frac{1}{2} W_{t_{0}}^2 - \\frac{1}{2} (t - t_0).\n",
      "\\end{equation}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we choose $V = X^2$ for the equation $dX_t = dW_t$ with $f = 0$ and $g = 1$ then we get\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "  && dX_t^2 & = dt + 2 X_t \\, dW_t. \\\\\n",
      "  \\implies && X_t^2 & = X_{t_0}^2 + \\int_{t_0}^t ds + \\int_{t_0}^t 2 X_s \\, dW_s \\\\\n",
      "  \\implies && W_t^2 & = W_{t_0}^2 + (t - t_0) + 2 \\int_{t_0}^t W_s \\, dW_s.\n",
      "\\end{align}\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exercise"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use the stochastic chain rule to prove that\n",
      "\n",
      "$$\n",
      "\\begin{equation}\n",
      "  dX_t = \\lambda X_t \\, dt + \\mu X_t \\, dW_t\n",
      "\\end{equation}\n",
      "$$\n",
      "\n",
      "has a solution of the form\n",
      "\n",
      "$$\n",
      "\\begin{equation}\n",
      "  X_t = X_0 \\exp \\left[ (\\lambda - \\tfrac{1}{2} \\mu) t + \\mu W_t \\right].\n",
      "\\end{equation}\n",
      "$$\n",
      "\n",
      "**Hint**: apply the chain rule to $V(x) = \\log(x)$."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}